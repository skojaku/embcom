\documentclass[12pt]{article} %{{{
\usepackage[margin=1in]{geometry}

% Figures
\usepackage{graphicx}
\graphicspath{{../../figs/}}

% Math
\usepackage{amsmath}
\usepackage{amssymb}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\usepackage{bm}
\usepackage{lmodern}% http://ctan.org/pkg/lm
\def\tnull{{\text{null}}}
\def\vec#1{{\bm #1}}
\def\diag{\text{diag}}
\def\mat#1{\mathbf{#1}}
\def\Xmat{{\bm X}}
\def\Exp{{\mathbb E}}
\def\Var{\text{Var}}

% abbreviations
\def\eg{e.g.,~}
\def\ie{i.e.,~}
\def\cf{cf.\ }
\def\viz{viz.\ }
\def\vs{vs.\ }
\author{Sadamori Kojaku}
% Refs
\usepackage{biblatex}
\addbibresource{../main.bib}

\usepackage{url}

\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\figref}[1]{Fig.~\ref{fig:#1}}
\newcommand{\tabref}[1]{Table~\ref{tab:#1}}
%\newcommand{\eqnref}[1]{\eqref{eq:#1}}
%\newcommand{\thmref}[1]{Theorem~\ref{#1}}
%\newcommand{\prgref}[1]{Program~\ref{#1}}
%\newcommand{\algref}[1]{Algorithm~\ref{#1}}
%\newcommand{\clmref}[1]{Claim~\ref{#1}}
%\newcommand{\lemref}[1]{Lemma~\ref{#1}}
%\newcommand{\ptyref}[1]{Property~\ref{#1}}

% for quick author comments
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\definecolor{light-gray}{gray}{0.8}
\def\del#1{ {\color{light-gray}{#1}} }
\def\yy#1{\footnote{\color{red}\textbf{yy: #1}} }

%}}}

% Remove line break after subsub section
\makeatletter
\renewcommand\subsubsection{\@startsection{subsubsection}{3}{\z@}%
                                     {-0.5ex\@plus -1ex \@minus -.2ex}%
                                     {-1.5ex \@plus -.2ex}% Formerly 1.5ex \@plus .2ex
                                     {\normalfont\normalsize\bfseries}}
\makeatother

\begin{document} %{{{

\title{Node on the detectability limit of the node2vec} %{{{
\date{\today}
\maketitle %}}}

\section{Set up}
\label{sec:introduction}

\subsection{Networks}

We consider a network generated from a stochastic block model (SBM) composed of $B=2$ blocks.
Each edge between nodes $i$ and $j$ appears with probability
\begin{align}
    p_{ij} = \left\{
    \begin{array}{cl}
        p_{\text{in}}  & \text{($i$ and $j$ belong to the same block)} \\
        p_{\text{out}} & \text{(otherwise)},
    \end{array}
    \right.
\end{align}
We assume that the network is sparse such that
\begin{align}
    n p_{\text{in}} = c_{\text{in}},\quad n p_{\text{out}} = c_{\text{out}},
\end{align}
where $n$ is the number of nodes in the network, and $c_{\text{in}}$ and $c_{\text{out}}$ are constant.

\subsection{node2vec matrix}

node2vec embeds a network by factorizing matrix $\mat{R}=(R_{ij})$~\cite{Qiu2018} that takes the same form as with the modularity matrix.
The matrix $\mat{R}=(R_{ij})$---which we refer to as the node2vec matrix---is given by
\begin{align}
    R_{ij} = \log{ \frac{1}{T}\sum_{t=1}^T P^{(t)}_{ij}} - \log \frac{d_j}{2M}, \label{eq:rij}
\end{align}
where $P^{(t)}_{ij}$ is the probability that a random walker at node $i$ moves to node $j$ by $t$ steps, $T$ is the window size (\ie a parameter of node2vec), and
$M$ is the number of edges in the network.
Assuming that the network is connected, there exists a stationary distribution for the random walks given by $\pi_i = d_i / 2M$,
where $\pi_i$ is the stationary probability at node $i$.
Using $\pi_i$, we rewrite $R_{ij}$ as
\begin{align}
    R_{ij} & = \log{ \frac{1}{T}\sum_{t=1}^T d_i P^{(t)}_{ij}} - \log \frac{d_i d_j}{2M}             \\
           & = \log{\frac{1}{T}\sum_{t=1}^T \frac{d_i}{2M} P^{(t)}_{ij}} - \log \frac{d_i d_j}{4M^2} \\
           & = \log{\frac{1}{T}\sum_{t=1}^T \pi_i P^{(t)}_{ij}} - \log \pi_i \pi_j                   \\
           & = \log{\tilde W_{ij}} - \log \pi_i \pi_j
\end{align}
where $\tilde W_{ij} = \pi_i P_{ij} ^{(t)}$ is the probability that, at the stationary state,
a random walker at $i$ visits $j$ in the next $T$ steps.
Because $\pi_i \pi_j$ corresponds to $\tilde W_{ij}$ under the configuration model, Eq.~\eqref{eq:rij} can be rewritten in a form similar to the modularity, \ie
\begin{align}
    \label{eq:modularity}
    R_{ij}  = \log{\tilde W_{ij}} - \log \Exp\left[ \tilde W_{ij} \right]_{\text{conf}},
\end{align}
where $\Exp\left[ \cdot \right]_{\text{conf}}$ is the expectation with respect to the configuration model.

\section{Stochastic properties of the node2vec matrix $\mat{R}$}

\subsection{Residual matrix $\mat{X}$}

Modularity matrix $\mat{R}$ by itself is a random matrix which can be rewritten as
\begin{align}
    \label{eq:modularity_2}
    \mat{R} =  \Exp\left[ \mat{R} \right]_{\text{SBM}} + \mat{X},
\end{align}
where $\Exp\left[ \cdot \right]_{\text{SBM}}$ is the expectation over the networks generated by the SBM, and $\mat{X}$ is a random matrix with mean $0$, which is given by
\begin{align}
    X_{ij} & = \log{\tilde W_{ij}} - \Exp\left[ \log \tilde W_{ij}  \right]_{\text{SBM}} \label{eq:Xij}.
\end{align}
What determines the detectability threshold for a modularity matrix is the spectral properties of $\mat{X}$~\cite{Nadakuditi2012}.
However, due to the non-linear transformation of random variable $\tilde W_{ij}$, it is difficult to analytically obtain $\mat{X}$'s spectral properties.

One can expect that $\tilde W_{ij}$ goes $0$ in the limit $n \rightarrow \infty$, meaning the chance that a random walker goes through the edge from $i$ to $j$ gets lower as the network becomes larger.
Consequently, mean $\Exp[\tilde W_{ij}]$ and variance $\Var[\tilde W_{ij}]$ also approach zero as $n\rightarrow \infty$.
If the variance $\Var[\tilde W_{ij}] \rightarrow 0$, Taylor expansion can be used to obtain a good approximation of the moments of $\log W_{ij}$.
Specifically, by taking Taylor expansion of $\log \tilde W_{ij}$ at $\Exp[\tilde W_{ij}]$, we have
\begin{align}
    \Exp\left[ \log \tilde W_{ij}\right]_{\text{SBM}} & \simeq \Exp\left[ \log \Exp\left[ \tilde W_{ij}\right]_{\text{SBM}} -\frac{\left(\tilde W_{ij}-\Exp\left[ \tilde W_{ij}\right]_{\text{SBM}}\right)^2}{2\Exp\left[ \tilde W_{ij}\right]^2 _{\text{SBM}}} \right]_{\text{SBM}} \nonumber \\
                                                      & = \log \Exp\left[ \tilde W_{ij}\right]_{\text{SBM}} -\frac{\Var\left[\tilde W_{ij}\right]_{\text{SBM}}}{2\Exp\left[ \tilde W_{ij}\right]^2 _{\text{SBM}}}.
\end{align}
Likewise, we obtain
\begin{align}
    \label{eq:varxij}
    \Var\left[ X_{ij}\right]_{\text{SBM}}:=\Var\left[ \log \tilde W_{ij}\right]_{\text{SBM}} & \simeq \frac{\Var\left[\tilde W_{ij}\right]_{\text{SBM}}}{\Exp\left[ \tilde W_{ij}\right]^2 _{\text{SBM}}}.
\end{align}
%which leads
%\begin{align}
%    X_{ij} & \simeq \log{\tilde W_{ij}} - \log \Exp\left[ \tilde W_{ij}\right]_{\text{SBM}} + \frac{\Var\left[\tilde W _{ij}\right]}{2\Exp\left[ \tilde W_{ij}\right]_{\text{SBM}} ^2} \nonumber \\
%           & = \log\left( \Exp\left[ \tilde W_{ij}  \right]_{\text{SBM}} + \tilde W_{ij} - \Exp\left[ \tilde W_{ij}  \right]_{\text{SBM}} \right) - \log \Exp\left[ \tilde W_{ij}\right]_{\text{SBM}} + \frac{\Var\left[\tilde W _{ij}\right]}{2\Exp\left[ \tilde W_{ij}\right]_{\text{SBM}} ^2} \nonumber \\
%           & = \log\left(1 + \frac{\tilde W_{ij} - \Exp\left[ \tilde W_{ij}  \right]_{\text{SBM}}}{\Exp\left[ \tilde W_{ij}  \right]_{\text{SBM}}} \right) + \frac{\Var\left[\tilde W _{ij}\right]}{2\Exp\left[ \tilde W_{ij}\right]_{\text{SBM}} ^2}.
%\end{align}
%We will show that $\frac{\tilde W_{ij} - \Exp\left[ \tilde W_{ij}  \right]_{\text{SBM}}}{\Exp\left[ \tilde W_{ij}  \right]_{\text{SBM}}}$ is very close to zero and thus
%\begin{align}
%    X_{ij} & \simeq \frac{\tilde W_{ij} - \Exp\left[ \tilde W_{ij}  \right]_{\text{SBM}}}{\Exp\left[ \tilde W_{ij}  \right]_{\text{SBM}}}+ \frac{1}{2\Exp\left[ \tilde W_{ij}\right]_{\text{SBM}} ^2}\left[\Var\left[\tilde W_{ij}\right] - \left( \tilde W_{ij} - \Exp\left[ \tilde W_{ij}  \right]_{\text{SBM}} \right)^2\right]
%\end{align}
%where we have exploited the approximation $\log (1 + x)\simeq x - x^2/2$ if $|x|$ is small.

\subsection{The first and second moments of $\tilde W_{ij}$}

Denoted by $A_{ij}=(a_{ij})$ the adjacency matrix generated by the SBM, where
$a_{ij} = 1$ or $a_{ij}=0$ indicates that nodes $i$ and $j$ are adjacent or not, respectively.
Assuming that the degree is fixed (which is indeed true in the limit $n\rightarrow \infty$), the transition probability from $i$ to $j$ by one step is given by
\begin{align}
    P_{ij} = \frac{a_{ij}}{d},
\end{align}
where $d$ is the expected degree of a node.
Note that all nodes have the same expected degree in our setting (See Set up subsection).
Bearing in mind that $P^{(t)}_{ij}$ is the probability that the random walker at node $i$ visits $j$ by $t$ steps,
we have
\begin{align}
    \label{eq:pij_random}
    P^{(t)} _{ij} = \frac{1}{d^t}\sum_{\ell_1, \ell_2\ldots \ell_{t-1}} a_{i,\ell_1}a_{\ell_2,\ell_3} \cdots a_{\ell_{t-1}j}
\end{align}
In other words, $P_{ij} ^{(t)}$ counts the paths of length $t$ that start $i$ and then end $j$.
Term $a_{i,\ell_1}a_{\ell_2,\ell_3} \cdots a_{\ell_{t-1}j}$ is a binary random variable, which takes $1$ if
all edges in the path appear in the network.
With the central limit theorem, $P^{(t)}_{ij}$ approximately follows a Gaussian distribution with mean $\mu_{ij}$ and variance $\sigma_{ij}$, \ie
\begin{align}
    P^{(t)} _{ij} \sim {\cal N}(\mu_{ij}(t), \sigma_{ij}(t) ). \label{eq:norm}
\end{align}
It is straightforward to derive the mean $\mu_{ij}(t)$.
Let us assume that the random walker does not move along the same edges more than once, which is true if the network is sparse but $d$ is sufficiently large.
Because $a_{ij}$ is independent of each other, the expected $P^{(t)}_{ij}$, denoted by $\Exp\left[ P^{(t)}_{ij}\right] $ is given by
\begin{align}
    \Exp\left[ P^{(t)} _{ij} \right]  =\frac{1}{d^t}\sum_{\ell_1, \ell_2\ldots \ell_{t-1}} \Exp\left[ a_{i,\ell_1} \right] \Exp\left[ a_{\ell_2,\ell_3} \right] \cdots \Exp\left[ a_{\ell_{t-1}j} \right], \label{eq:exp_pij}
\end{align}
were we omit the subscript $[\cdot]_\text{SBM}$.
An important consequence of Eq.~\eqref{eq:exp_pij} is that we can easily calculate the expected power $\Exp\left[ \mat{P}^{(t)} \right]$ from $\Exp\left[ \mat{P} \right]$, \ie
\begin{align}
    \Exp\left[ \mat{P}^{(t)} \right]  = \Exp\left[ \mat{P} \right] ^t.
\end{align}
To calculate the power $\Exp\left[ \mat{P} \right] ^t$, we rewrite $\Exp\left[ \mat{P} \right]$ as follows.
Remind that $\Exp\left[ P_{ij} \right] = p_\text{in}/d$ or $= p_\text{out}/d$ if nodes $i$ and $j$ belong
to the same block or not, respectively, and
degree $d = (c_{\text{in}} + c_{\text{out}})/2$.
Therefore, we rewrite $\Exp\left[ P_{ij} \right]$ as
\begin{align}
    \Exp\left[ \mat{P} \right] = \mat{\Theta} \mat{\Lambda}\mat{\Theta}^\top  \frac{2}{n},\quad
    \mat{\Lambda} =
    \frac{1}{c_{\text{in}} + c_{\text{out}}}\left[
        \begin{array}{cc}
            c_{\text{in}}  &
            c_{\text{out}}   \\
            c_{\text{out}} &
            c_{\text{in}}
        \end{array}
        \right],
\end{align}
where $\mat{\Theta} = (\Theta_{ik})$ is an $N\times 2$ membership matrix, \ie
\begin{align}
    \Theta_{ik} = \left\{
    \begin{array}{cl}
        1 & \text{(if node $i$ belongs to block $k$)} \\
        0 & \text{(otherwise)}.
    \end{array}
    \right.
\end{align}
The power $\Exp\left[ \mat{P}^{(t)} \right]$ can be computed by
\begin{align}
    \Exp\left[ \mat{P} \right] ^t & =
    \left( \frac{2}{n}\mat{\Theta} \mat{\Lambda} \mat{\Theta}^\top\right) \left(\frac{2}{n}\mat{\Theta} \mat{\Lambda} \mat{\Theta}^\top \right)\Exp\left[ \mat{P} \right] ^{t-2} \nonumber \\
                                  & = \frac{2}{n}\mat{\Theta} \mat{\Lambda}^2 \mat{\Theta}^\top \Exp\left[ \mat{P} \right] ^{t-2} \nonumber                                                \\
                                  & = \frac{2}{n}\mat{\Theta} \mat{\Lambda}^t \mat{\Theta}^\top.
\end{align}
or equivalently,
\begin{align}
    \mu_{ij}(t)
     & = \frac{2}{n}\left(\mat{\Lambda}^t \right)_{g_i, g_j}.
\end{align}
where $g_i \in \{0, 1\}$ is the membership of node $i$, and we have used $\mat{\Theta}^\top \mat{\Theta} = \frac{n}{2}\mat{I}$.

Let us shift our focus onto variance $\sigma_{ij}(t)^2$ for $P_{ij} ^{(t)}$.
Remind that $P_{ij} ^{(t)}$ consists of a sum of random variables $a_{i,\ell_1}a_{\ell_2,\ell_3} \cdots a_{\ell_{t-1}j}$ (Eq.~\eqref{eq:norm}).
Each $a_{i,\ell_1}a_{\ell_2,\ell_3} \cdots a_{\ell_{t-1}j}$ is a Bernoulli random variable that is \textit{dependent} of one another.
This dependency makes it difficult to analytically compute the variance.
Yet, we can compute the upper bound of the variance by assuming that each random variable has the same flipping probability $\theta_{ij}$ and is independent of each other.

With this idea in mind, let us derive the upper bound of $\sigma_{ij} ^{(t)}$.
If all $a_{i,\ell_1}a_{\ell_2,\ell_3} \cdots a_{\ell_{t-1}j}$ are independent and have the same flipping probability $\theta_{ij}$, we have
\begin{align}
    \Exp\left[ P_{ij}^{(t)} \right] = \frac{n^{t-1}}{d^t}\theta_{ij} \;
    \Rightarrow\; \theta_{ij}  = \Exp\left[ P_{ij}^{(t)} \right] \frac{d^t}{n^{t-1}}.
\end{align}
The variance for $P_{ij}^{(t)}$ is then given by
\begin{align}
    \label{eq:variance_2}
    \sigma_{ij}(t)^2 = \Exp\left[ \left( P_{ij}^{(t)} - \Exp\left[ P_{ij}^{(t)} \right] \right)^2 \right]
     & \leq \frac{n^{t-1}}{d^{2t}}\theta_{ij}(1-\theta_{ij})
    \leq \frac{n^{t-1}}{d^{2t}}\theta_{ij}                         \\
     & = \frac{\Exp\left[ P_{ij}^{(t)} \right]}{d^t}               \\
     & = \frac{ 2\left(\mat{\Lambda} ^t \right)_{g_i, g_j}}{n d^t}
\end{align}

Taken together, $\Exp[\tilde W_{ij}]_{\text{SBM}}$ and (the upper bound of) $\Var[\tilde W_{ij}]_{\text{SBM}}$ are given by
\begin{align}
    \Exp\left[ \tilde W_{ij} \right]_{\text{SBM}} & = \frac{1}{T} \sum_{t=1}^T \mu_{ij}(t)
    = \frac{2}{nT} \left( \sum_{t=1}^T \Exp\left[ \mat{\Lambda}\right]^t\right)_{g_i, g_j}, \label{eq:ewij} \\
    \Var\left[\tilde W_{ij}\right]_{\text{SBM}}   & = \frac{1}{T^2} \sum_{t=1}^T \sigma_{ij}(t)^2
    = \frac{1}{nT^2} \sum_{t=1}^T \frac{ 2\left(\mat{\Lambda} ^t \right)_{g_i, g_j}}{d^t}.  \label{eq:vwij}
\end{align}

\subsection{Validation}
See Fig.~\ref{fig:validation}.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{rvals.pdf}
    \caption{
        The first and second moment of $\tilde W_{ij}$.
        We calculate them using Eqs.~\eqref{eq:ewij} and \eqref{eq:vwij}.
        Additionally, we calculate the moments by
        generating the 30 networks with the SBM and computing the statistics.
        We set parameters by $c_{\text{out}} = 30$, $c_{\text{in}} = 5$.
        ({\bf A--D}) $\Exp[\tilde W_{ij}]$.
        ({\bf E--H}) $\Var[\tilde W_{ij}]$.
        Eqs.~\eqref{eq:ewij} and \eqref{eq:vwij} are close to those calculated by the simulations.
    }
    \label{fig:validation}
\end{figure}

\section{Detectability threshold of the node2vec matrix}

Using Eqs.~\eqref{eq:varxij} and ~\eqref{eq:variance_2}, we have
\begin{align}
    \Var\left[ X_{ij}\right]_{\text{SBM}}
     & \simeq \frac{\Var\left[\tilde W_{ij}\right]_{\text{SBM}}}{\Exp\left[ \tilde W_{ij}\right]^2 _{\text{SBM}}}
    = \frac{
        \frac{1}{nT^2} \sum_{t=1}^T \frac{ 2\left(\mat{\Lambda} ^t \right)_{g_i, g_j}}{d^t}
    }{
        \frac{4}{T^2n^2} \left( \sum_{t=1}^T \Exp\left[ \mat{\Lambda}\right]^t\right)_{g_i, g_j} ^2
    } \nonumber                                                                                                   \\
     & = n \cdot \frac{
        \sum_{t=1}^T d^{-t}\left(\mat{\Lambda} ^t \right)_{g_i, g_j}
    }{
        2\left( \sum_{t=1}^T \Exp\left[ \mat{\Lambda}\right]^t\right)_{g_i, g_j} ^2
    },
\end{align}
which means if $n \rightarrow \infty$ then $\Var[X_{ij}] \rightarrow \infty$.

\subsection{Validation}

\begin{figure}
    \centering
    \includegraphics[width=\hsize]{two-coms-auc.pdf}
    \caption{
        Similarity between and within communities.
        We compute the Area Under Curve of the receiving operator characteristics (AUC) for
        the inter- and intra-node similarity measured by the cosine similarity and euclidean distance.
        A higher AUC means that the communities are clustered and well separated with each other.
        As $n$ increases, the AUC decreases, suggesting that node2vec and deepwalk fail to capture the communities in large sparse networks.
    }
    \label{fig:two_com_n}
\end{figure}

\subsubsection*{Network Properties}

\begin{enumerate}
    \item Degree is homogeneous, \ie $\Exp\left[ d_i \right]> = (c_{\text{in}} + c_{\text{out}}) / 2$.
    \item $n \rightarrow \infty \Rightarrow p_{\text{out}}, p_{\text{in}}, p \rightarrow 0$
\end{enumerate}

%\section{Results}\label{sec:results} %{{{


%}}}

%\section{Methods}\label{sec:methods} %{{{

%}}}

\printbibliography{}

\end{document} %}}}
