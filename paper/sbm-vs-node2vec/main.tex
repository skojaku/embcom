\documentclass[12pt]{article} %{{{
\usepackage[margin=1in]{geometry}

% Figures
\usepackage{graphicx}
\graphicspath{{../../figs/}}

% Math
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage[usestackEOL]{stackengine}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\diag}{diag}
\def\given{\mid}


% abbreviations
\def\eg{e.g.,~}
\def\ie{i.e.,~}
\def\cf{cf.\ }
\def\viz{viz.\ }
\def\vs{vs.\ }

% math notation
\usepackage{bm}
\usepackage{lmodern}% http://ctan.org/pkg/lm
\usepackage{xspace}

\def\tnull{{\text{null}}}
\def\vec#1{{\bm #1}}
\def\diag{\text{diag}}
\def\mat#1{\mathbf{#1}}
\def\Xmat{{\bm X}}
\def\Exp{{\mathbb E}}
%\newcommand\given[1][]{\:#1\vert\:}
\newcommand{\Na}{N_{\rm a}}%{\mathrm{J}}
\newcommand{\Nf}{N_{\rm f}}%{\mathrm{J}}
\def\ErdosRenyi{{Erd\H{o}s-R\'{e}nyi}\xspace}

% Refs
\usepackage{biblatex}
\addbibresource{../main.bib}

\usepackage{url}

\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\figref}[1]{Fig.~\ref{fig:#1}}
\newcommand{\tabref}[1]{Table~\ref{tab:#1}}
%\newcommand{\eqnref}[1]{\eqref{eq:#1}}
%\newcommand{\thmref}[1]{Theorem~\ref{#1}}
%\newcommand{\prgref}[1]{Program~\ref{#1}}
%\newcommand{\algref}[1]{Algorithm~\ref{#1}}
%\newcommand{\clmref}[1]{Claim~\ref{#1}}
%\newcommand{\lemref}[1]{Lemma~\ref{#1}}
%\newcommand{\ptyref}[1]{Property~\ref{#1}}

% for quick author comments
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\definecolor{light-gray}{gray}{0.8}
\definecolor{light-blue}{RGB}{33,150,243}
\definecolor{light-green}{RGB}{76, 175, 80}
\def\del#1{ {\color{light-gray}{#1}} }
\def\yy#1{\footnote{\color{red}\textbf{yy: #1}} }
\def\sada#1{\footnote{\color{light-blue}\textbf{sk: #1}} }
\def\jisung#1{\footnote{\color{light-green}\textbf{js: #1}} }

%}}}


\begin{document} %{{{

\title{On node2vec and stochastic block models} %{{{
\date{\today}
\author{Sadamori Kojaku}
\maketitle %}}}

\section*{(Tentative) Summary}

Graph embedding is a powerful tool to embed a network onto a vector space that allows us to compute the relationships of nodes using a simple vector operation.
A widespread application is community detection.
While graph embedding has been successful in capturing communities in networks, it is not clearly understood why and when graph embedding works.
Here, we analyze the relationships of \textit{node2vec}---a widespread graph embedding---and the stochastic block model for networks (SBM).
We show that the embedding generated by \textit{node2vec} reflects the communities in degree-corrected SBM such as the Markov stability of communities.
We also show the equivalence of clustering of nodes on embeddings and community detection for networks based on normalized cut.
%Furthermore, the condition for perfect clustering hinges upon Markov stability of communities.
(speculation) Additionally, we argue (i) a need for using dot similarity instead of Euclidean distance to improve performance of community detection, and (ii)
that clustering on embeddings does not have resolution limit thanks to the continuous vector representation of communities.

\tableofcontents

\section{\textit{node2vec}}
\label{sec:node2vec}

Many graph embedding methods such as \textit{node2vec}~\cite{Grover2016}, \textit{DeepWalk}~\cite{Perozzi2014}, and \textit{metapath2vec}~\cite{Dong2017} hinge upon word embedding methods, a widespread model being \textit{word2vec}~\cite{Mikolov2013}.
These graph embedding methods employ random walks to generate a sequence of nodes that a walker visits in a network, which is then fed as ``sentence'' to {\it word2vec} to generate a node embedding.
Here we focus on a widespread graph embedding, \textit{node2vec}.

\subsection{Model}

Consider a sentence of nodes, denoted by ($x_{1}, x_{2}, \ldots, x_{T}$), where $x_{t}$ is the $t$th node in the sentence.
\textit{word2vec} associates node $i$ with context nodes
\begin{equation}
    x_{t-L}, \ldots, x_{t-1}, x_{t+1},\ldots, x_{t+L},
\end{equation}
that appear in the surrounding of $x_t$, with a time lag up to $L$, where $L$ is the size of the context window.
Then, skip-gram \textit{word2vec}---a variant of \textit{word2vec}---models the conditional probability $P(x_{t + \tau}=j \vert x_{t} = i)$ that context $x_{t + \tau} = j$ appears in the surrounding of node $x_{t} = i$ by
\begin{equation}
    P^{\text{w2v}}(x_{t + \tau}=j \vert x_{t} = i) = \frac{\exp(\vec{u}_i ^\top \vec{v}_{j})}{Z_i}, \label{eq:cond_prob_w2v}
\end{equation}
where $Z_i=\sum_{j'=1}^N \exp(\vec{u}_{i}^\top \vec{v}_{j'})$ is a normalization constant, $N$ is the number of all unique nodes, and $\vec{u}_i$ and $\vec{v}_j$ are the column vectors of length $N$ representing embedding vectors for node $i$ and $j$.
The two embedding vectors $\vec{u}_i$ and $\vec{v}_i$ can be different and capture the characteristics of node $i$ as a center and a context node, respectively.
Given a sentence of length $T$, skip-gram \textit{word2vec} finds the embedding vectors by maximizing the log-likelihood
\begin{align}
    \sum_{t = 1}^{T} \sum_{\substack{-L \leq \tau \leq L \\ \tau \neq 0 \\ 0\leq t+\tau \leq T }} \log P^{\text{w2v}}(x_{t + \tau} \vert x_{t}), \nonumber
\end{align}
or equivalently,
\begin{align}
       {\cal J} := \sum_{i=1}^N \sum_{j=1}^N P^{\text{data}}(i, j) \log P^{\text{w2v}}(j \vert i), \label{eq:word2vec-obj}
\end{align}
where $P^{\text{data}}(i,j)$ is the empirical co-occurrence probability for center node $i$ and context node $j$ calculated from the given sentence, \ie
\begin{align}
    P^{\text{data}}(i,j) = c \sum_{t = 1}^{T} \sum_{\substack{-L \leq \tau \leq L \\ \tau \neq 0 \\ 0\leq t+\tau \leq T }} \delta(x_{t + \tau},j)\delta(x_{t}, i),
\end{align}
where $\delta$ is the Kronecker delta, and $c$ is the normalization constant to ensure $\sum_{i=1}^N \sum_{j=1}^N P^{\text{data}}(i,j)=1$.

\subsection{Training \textit{word2vec}}
\label{sec:training}

Maximizing ${\cal J}$ is computationally demanding because $P^{\text{w2v}}$ involves the sum over all nodes.
To circumvent this issue, several heuristics have been proposed~\cite{Mikolov2013,Perozzi2014,Grover2016}.
Among many heuristics, node2vec employs negative sampling to train \textit{word2vec}.
%However, negative sampling is in fact a biased estimator~\cite{Gutmann2010,Dyer2014} that does not find the optimal embedding that maximizes ${\cal J}$.
%While the negative sampling does not fit \textit{word2vec} correctly, this statistical bias has a critical role in the success of \textit{word2vec}.

Negative sampling trains \textit{word2vec} as follows.
Given a sentence, a center-context node pair $(i,j)$ is sampled and labeled as $Y_j=1$.
Then, for the same center node $i$, $k$ context nodes $\ell$ are sampled from a noise distribution $p_0(\ell)$ and labeled as $Y_{\ell}=0$.
The noise distribution is given by $p_0(\ell) \propto w^\gamma _\ell$, where $w_\ell$ is the fraction of node $j$ in the sentence, and $\gamma$ is a hyper-parameter.
Negative sampling fits a logistic regression model that predicts $Y_j$ for a node pair $(i,j)$, \ie
\begin{align}
    \label{eq:logistic-regress}
    P^{\text{NS}}(Y_{j} = 1; \vec{u}_{i}, \vec{v}_j) = \frac{1}{1 + \exp(-\vec{u}_i ^\top \vec{v}_{j})},
\end{align}
by maximizing the log-likelihood:
\begin{align}
    \label{eq:log-likelihood-logistic-regress}
    {\cal J}^{\text{NS}} = \frac{1}{|{\cal D}|}\sum_{(i,j) \in {\cal D}} \left[ \log P^{\text{NS}}(Y_{j} = 1; \vec{u}_{i}, \vec{v}_j) + k\sum_{\ell=1}^N p_0(\ell) \log P^{\text{NS}}(Y_{\ell} = 0; \vec{u}_{i}, \vec{v}_\ell)\right],
\end{align}
where ${\cal D}$ is the set of node pairs taken from the given sentence.

\subsection{Optimal embedding for \textit{node2vec}}

Optimizing ${\cal J}^{\text{NS}}$ is a non-convex optimization problem and hence is challenging in practice.
However, it is possible to find an optimal embedding for ${\cal J}^{\text{NS}}$ if we have a sufficiently large dimension~\cite{Levy2014}.
The objective ${\cal J}^{\text{NS}}$ of negative sampling can be rewritten as
\begin{align}
    \label{eq:Jns_1}
    {\cal J}^{\text{NS}} & = \frac{1}{|{\cal D}|}\sum_{(i,j) \in {\cal D}} \log P^{\text{NS}}(Y_{j} = 1; \vec{u}_{i}, \vec{v}_j) + \frac{k}{|{\cal D}|}\sum_{(i,j) \in {\cal D}} \sum_{\ell=1}^N p_0(\ell) \log P^{\text{NS}}(Y_{\ell} = 0; \vec{u}_{i}, \vec{v}_\ell) \nonumber \\
                         & = \sum_{i=1}^N \sum_{j=1}^N w_{ij} \log P^{\text{NS}}(Y_{j} = 1; \vec{u}_{i}, \vec{v}_j) + k\sum_{i=1}^N \sum_{j=1}^N w_{ij}\sum_{\ell=1}^N p_0 (\ell)\log P^{\text{NS}}(Y_{\ell} = 0; \vec{u}_{i}, \vec{v}_\ell),
\end{align}
where $w_{ij}$ is the co-occurrence probability of nodes $i$ and $j$ in the given sentence, or equivalently, $w_{ij}=\sum_{(i,j) \in {\cal D}} 1 / |{\cal D}|$.
By substituting $\sum_{j=1}^N w_{ij}=w_i$ and $p_0(\ell) = c \cdot w^{\gamma} _i$ into Eq.~\eqref{eq:Jns_1}, we have
\begin{align}
{\cal J}^{\text{NS}}
                         & = \sum_{i=1}^N \sum_{j=1}^N w_{ij} \log P^{\text{NS}}(Y_{j} = 1; \vec{u}_{i}, \vec{v}_j) + k\sum_{i=1}^N w_i\sum_{\ell=1}^N cw^\gamma _\ell \log P^{\text{NS}}(Y_{\ell} = 0; \vec{u}_{i}, \vec{v}_\ell) \nonumber \\
                         & = \sum_{i=1}^N \sum_{j=1}^N w_{ij} \log \frac{1}{1 + \exp(-\vec{u}^\top _i \vec{v}_j)} + kc\sum_{i=1}^N w_i\sum_{\ell=1}^N w^\gamma _\ell \log \frac{\exp(-\vec{u}^\top _i \vec{v}_\ell)}{1 + \exp(-\vec{u}^\top _i \vec{v}_\ell)}.
\end{align}
where $c:=1/\sum_{\ell=1}^N P^\gamma (\ell) $ is the normalization constant to ensure $\sum_{\ell=1}^N p_0(\ell)=1$.
In Ref.~\cite{Levy2014}, hyperparameter $\gamma$ is set to 1 to keep the analysis simple, which yields
\begin{align}
    {\cal J}^{\text{NS}}
                         & = \sum_{i=1}^N \sum_{j=1}^N w_{ij} \log \frac{1}{1 + \exp(-\vec{u}^\top _i \vec{v}_j)} + k\sum_{i=1}^N w_{i}\sum_{\ell=1}^N w_\ell \log \frac{\exp(-\vec{u}^\top _i \vec{v}_\ell)}{1 + \exp(-\vec{u}^\top _i \vec{v}_\ell)}.
\end{align}
Now, imagine we have $K=N$ dimensions for an embedding of $N$ nodes.
Just like an $N$th order polynomial function can be fit to $N$ points without errors, the embedding model is fit to the given data without errors if $K=N$ because any $N\times N$ pairwise dot similarities can be expressed by a linear combination of $N$ basis vectors.
Crucially, with $K=N$ dimensions, one can move nodes in space such that only $\vec{u}_i ^\top \vec{v}_j$ is changed while other similarities being fixed.
Therefore, ${\cal J}^{\text{NS}}$ can be reparameterized by
\begin{align}
    {\cal J}^{\text{NS}}
                         & = \sum_{i=1}^N \sum_{j=1}^N w_{ij} \log \frac{1}{1 + \exp(-r_{ij})} + k\sum_{i=1}^N w_{i}\sum_{\ell=1}^N w_\ell \log \frac{\exp(-r_{i\ell})}{1 + \exp(-r_{i\ell})},
\end{align}
where $r_{ij}=\vec{u}_i ^\top \vec{v}_j$.
By taking a derivative of ${\cal J}^{\text{NS}}$ with respect to $r_{ij}$, we have
\begin{align}
    \frac{\partial {\cal J}^{\text{NS}}}{\partial r_{ij}} &= w_{ij}\left(1-\frac{1}{1 + \exp(-r_{ij})}\right) - k w_i w_j\frac{1}{1 + \exp(-r_{ij})}.
\end{align}
Solving $\partial {\cal J}^{\text{NS}} /\partial r_{ij} = 0$ with respect to $r_{ij}$ leads
\begin{align}
    \label{eq:opt_dotsim_sentence}
    &\frac{\partial {\cal J}^{\text{NS}}}{\partial r_{ij}} = w_{ij}\left(1-\frac{1}{1 + \exp(-r_{ij})}\right) - k w_i w_j\frac{1}{1 + \exp(-r_{ij})} = 0 \nonumber \\
    &\Rightarrow \left(w_{ij}+ k w_i w_j\right)\frac{1}{1 + \exp(-r_{ij})} = w_{ij} \nonumber \\
    &\Rightarrow \frac{1}{1 + \exp(-r_{ij})} = \frac{w_{ij}}{w_{ij}+ k w_i w_j} \nonumber \\
    &\Rightarrow 1 + \exp(-r_{ij}) = 1+ \frac{k w_i w_j}{w_{ij}} \nonumber \\
    &\Rightarrow r_{ij} = \ln \frac{w_{ij}}{w_i w_j} - \ln k.
\end{align}
There are two key features of Eq.~\eqref{eq:opt_dotsim_sentence}.
The first is that, when $K=N$, the dot similarity $\vec{u}_i ^\top \vec{v}_j$ for the optimal embedding is equal to $r_{ij}$.
The second is that the dot similarity is symmetric, \ie $r_{ij}=r_{ji}$, suggesting that the in-vector is equal to the out-vector, \ie $\vec{u}_i = \vec{v}_i$.
While embedding dimension $K$ is much smaller than $N$ ($K\ll N$) in practice, the gap between the results in practice and theory will be filled as the dimension $K$ increases.


\subsection{Sentences generated by random walks}

The dot similarity $r_{ij}$ is determined by two statistics of the sentence generated by random walks, \ie $w_{ij}$ and $w_{i}$.
We can compute $w_{ij}$ and $w_{i}$ directly from the network by exploiting the well-known properties of random walks.
The co-occurrence probability $w_{ij}$ can be decomposed into
\begin{align}
    \label{eq:jgiveni}
    w_{ij} = w_i P^{\text{data}}(j \given i),
\end{align}
where $P^{\text{data}}(j\given i)$ is the probability that node $j$ appears as the context given that node $i$ is the center node in the sentence.
We rewrite the conditional probability $P^{\text{data}}(j \given i)$ using the transition probability of the random walks as
\begin{align}
    P^{\text{data}}(j \given i) & = \frac{1}{2L} \sum_{\ell = -L, \ell\neq 0}^L P^{\text{rw}}(x_{t+\ell}=j\given x_{t} = i),
\end{align}
where $P^{\text{rw}}(x_{t'}=j \given x_{t}=i)$ is the probability that a walker visits $j$ at time $t'$ given that it visits $i$ at time $t$.
It is straightforward to calculate the probability of visiting $j$ \textit{after} $i$, \ie
\begin{align}
    P^{\text{rw}}(x_{t+\ell}=j\given x_{t} = i) = (\mat{T}^\ell)_{ij},\; \forall \ell\geq 1, \label{eq:jgiveni_after}
\end{align}
where $\mat{T}$ is an $N\times N$ matrix with entry $T_{ij}$ indicating the probability of moving from $i$ to $j$ with one step.

Now, let us calculate the conditional probability of visiting $j$ {\it before} $i$ using $\mat{T}$.
To this end, we exploit the relationship between the conditional and joint probabilities, \ie
\begin{align}
    P^{\text{rw}}(x_{t-\ell}=j, x_{t} = i) & = P^{\text{rw}}(x_{t} = i \given x_{t-\ell}=j)P^{\text{rw}}(x_{t-\ell} = j) \nonumber         \\
                                           & = P^{\text{rw}}(x_{t-\ell} = j \given x_{t}=i)P^{\text{rw}}(x_{t} = i), \label{eq:cond-joint}
\end{align}
where $P^{\text{rw}}(x_{t} = i)$ is the probability of visiting $i$ at time $t$.
We assume that the random walk reaches a stationary state, in which $P^{\text{rw}}(x_{t-\ell} = j)$ is time-invariant, \ie $P^{\text{rw}}(x_{t-\ell} = j) = P^{\text{rw}}(x_{t} = j)$.
This assumption is based on a premise that the network is strongly connected and the random walk is aperiodic, which are always true for connected undirected networks.
For directed networks, there are several widespread tricks to ensure the stationarity, for example, introducing a teleportation probability to random walks~\cite{Lambiotte2012}.
In the following, we denote by $\pi_i$ the stationary probability at $i$ for short.

In stationary state, node $i$ appears with probability $\pi_i$. Therefore, the fraction $w_i$ of node $i$ in a sentence is given by
\begin{align}
    \label{eq:wi_graph}
    w_i = \pi_i.
\end{align}
Rearranging Eq.~\eqref{eq:cond-joint}, substituting Eq.~\eqref{eq:jgiveni_after}, and exploiting stationarity ($P^{\text{rw}}(x_{t-\ell} = j) = P^{\text{rw}}(x_{t} = j)=\pi_j$) yield
\begin{align}
    P^{\text{rw}}(x_{t-\ell}=j\given x_{t} = i) & = \frac{P^{\text{rw}}(x_{t-\ell} = j)P^{\text{rw}}(x_{t} = i \given x_{t-\ell}=j)}{P^{\text{rw}}(x_{t}=i)} \nonumber              \\
                                                & =\frac{\pi_j}{\pi_i}(\mat{T}^\ell)_{ji},\; \forall \ell\geq 1. \label{eq:jgiveni_before}
\end{align}
Combining Eqs.~\eqref{eq:jgiveni_after} and \eqref{eq:jgiveni_before}, conditional probability $P^{\text{data}}(j \given i)$ can be rewritten using ${\bf T}$ as
\begin{align}
    \label{eq:jgiveni_graph}
    P^{\text{data}}(j \given i) & = \frac{1}{2L}\left(\sum_{\ell=1}^L \mat{T}^\ell\right)_{ij} + \frac{\pi _j}{2L\pi _i}\left( \sum_{\ell=1}^L \mat{T}^\ell\right)_{ji}. %\nonumber \\
\end{align}
By substituting Eqs.~\eqref{eq:jgiveni}, \eqref{eq:wi_graph} and \eqref{eq:jgiveni_graph} into Eq.~\eqref{eq:opt_dotsim_sentence}, the dot similarity $r_{ij}$ can be described using the transition matrix $\mat{T}$ as
\begin{align}
    \label{eq:opt_dotsim_graph}
    r_{ij} = \ln \left( \frac{1}{2L}\left(\sum_{\ell=1}^L \mat{T}^\ell\right)_{ij} + \frac{\pi _j}{2L\pi _i}\left( \sum_{\ell=1}^L \mat{T}^\ell\right)_{ji} \right) - \ln \pi_j -\ln k
\end{align}
Equation~\eqref{eq:opt_dotsim_graph} can be simplified if the network is undirected.
The stationary distribution satisfies the detailed balanced condition
\begin{align}
    \label{eq:detailed_balanced}
    \pi_i T_{ij} = \pi_j T_{ji},
\end{align}
or equivalently,
\begin{align}
    \label{eq:detailed_balanced_2}
    \pi_i (\mat{T}^\ell)_{ij} = \pi_j (\mat{T}^\ell)_{ji}.
\end{align}
Rearranging Eq.~\eqref{eq:detailed_balanced_2} yields
\begin{align}
    \label{eq:detailed_balanced_3}
    \frac{\pi_j}{\pi_i} (\mat{T}^\ell)_{ji}  = (\mat{T}^\ell)_{ij} .
\end{align}
In undirected networks, stationary probability $\pi_i$ is proportional to the strength $s_i$ of node $i$ (\ie the sum of the weight of edges emanating from node $i$)~\cite{Masuda2017}, \ie
\begin{align}
    \label{eq:pi_graph}
    \pi_i = \frac{s_i}{2S},
\end{align}
where $S = \sum_{i=1}^N s_i /2$ is the sum of weights of all edges in the network.
By substituting Eqs.~\eqref{eq:detailed_balanced_3} and \eqref{eq:pi_graph} into Eq.~\eqref{eq:jgiveni_graph}, we have
\begin{align}
    \label{eq:opt_dotsim_undirected_graph}
    r_{ij} = \ln \left(\frac{1}{L}\sum_{\ell=1}^L \mat{T}^\ell\right)_{ij} - \ln s_j -\ln \frac{k}{2S},
\end{align}
if the given network is undirected.

\section{Stochastic block model}
\label{sec:sbm}

\subsection{Model}

We consider a network generated by stochastic block models (SBM)~\cite{Karrer2011}.
In SBM, each node belongs to a block (\ie group of nodes), and edge are placed from node $i$ to node $j$ with a probability following the Poisson distribution with mean
\begin{align}
    \label{eq:dcsbm}
    \langle \tilde w_{ij} \rangle = \lambda_{b_i,b_j} \theta^{+}_i \theta^{-}_j,
\end{align}
where $\tilde w_{ij}$ is the weight of the edge from node $i$ to node $j$, $\langle\cdot\rangle$ indicates the expectation over the Poisson distribution, $b_i$ and $b_j$ represent the blocks to which nodes $i$ and $j$ belong, respectively, and $\lambda_{b_i,b_j}$ is the edge propensity between blocks $b_i$ and $b_j$.
Parameter $\theta_i ^+$ is the propensity of node $i$ to provide edges, and $\theta_j ^-$ is the propensity of node $j$ to receive edges, which are normalized such that
\begin{align}
    \sum_{i=1, b = b_i}^N \theta_i ^+  =\sum_{i=1, b = b_i}^N \theta_i ^- = 1,\; \forall 1\leq b \leq B. \label{eq:normalization-constraint}
\end{align}
There are many normalization to ensure Eq.~\eqref{eq:normalization-constraint}, most common ones being
\begin{align}
    \label{eq:node_propensity}
    \theta_i ^{\pm} = \left\{
        \begin{array}{ll}
            1/n_{b_i} & \text{(for degree-uncorrected SBM)} \\
            s_i ^\pm / S_{b_i}^{\pm} & \text{(for degree-corrected SBM)},
        \end{array}
    \right.
\end{align}
where $n_b$ is the number of nodes in block $b$, $s^{+}_i$ and $s^{-}_i$ are the sum of the weights of all out-going and in-coming edges for node $i$ (i.e., out-strength and in-strength), respectively, and
$S_b ^{\pm} = \sum_{i=1}^N s_i ^{\pm} \delta(b, b_i)$ is the sum of $s^{\pm}_i$ over all nodes in block $b$~\cite{Karrer2011}.

\subsection{Random walks in the stochastic block model}

For a network with the expected edge weights, the random walker moves from $i$ to $j$ with probability
\begin{align}
    \label{eq:trans-prob-dcSBM-1}
    T^{\text{SBM}} _{ij} & = \frac{\langle\tilde w_{ij}\rangle}{\sum_{j'=1}^N \langle\tilde w_{ij'}\rangle} \nonumber \\
                         & = \frac{\lambda_{b_i,b_j} \theta^{+}_i \theta^{-}_j}{\sum_{j'=1}^N \lambda_{b_i,b(j')} \theta^{+}_i \theta^{-}_{j'}} \nonumber \\
                         & = \frac{\lambda_{b_i,b_j} \theta^{-}_j}{\sum_{j'=1}^N \lambda_{b_i,b(j')} \theta^{-}_{j'}} \nonumber \\
                         & = \frac{\lambda_{b_i,b_j} \theta^{-}_j}{\sum_{b=1}^B \lambda_{b_i,b} \left[\sum_{j'=1}^N\theta^{-}_{j'}\delta(b, b(j'))\right]}
                         %& = \left[ \diag\left(S^{\text{out}}_1, \ldots, S^{\text{out}}_K \right)^{-1}  \mat{\Lambda}\right]_{b_i,b_j} \frac{s^{\text{in}}_j}{S^{\text{in}} _{b_j}}.
\end{align}
Substituting Eq.~\eqref{eq:normalization-constraint} into Eq.~\eqref{eq:trans-prob-dcSBM-1} yields
\begin{align}
    \label{eq:Tsbm}
    T^{\text{SBM}} _{ij} &= \frac{\lambda_{b_i,b_j}}{\sum_{b=1}^B \lambda_{b_i,b}} \cdot  \theta^{-}_j = \overline \lambda_{b_i, b_j} \cdot  \theta^{-}_j,
\end{align}
where $\overline \lambda_{b_i, b_j}:= \lambda_{b_i,b_j} / \sum_{b=1}^B \lambda_{b_i,b}$ is the normalized edge propensity.

Equation~\eqref{eq:Tsbm} provides an intuitive interpretation regarding random walks in SBM.
Normalized edge propensity $\overline \lambda_{b, b'}$ can be interpreted as the transition probability of the walker from block $b$ to block $b'$ because it is normalized such that $\sum_{b'=1}^B \overline \lambda_{b, b'}=1$.
Node propensity $\theta^{-} _j$ can be also interpreted as a probability of sampling node $j$ from block $b_j$ because $\theta^{-} _j$ is normalized such that $\sum_{j'=1, b_j=b(j')}^N \theta^{-} _{j'}= 1$ (\ie Eq.~\eqref{eq:normalization-constraint}).
Therefore, transition probability $T^{\text{SBM}} _{ij}$ is decomposed into (i) the transition probability $\lambda_{b_i, b'}$ of the walker at the block level, and (ii) the probability $\theta^- _j$ of sampling node $j$ from nodes in block $b'$.

With this interpretation in mind, let us consider a random walker taking $\ell$ steps.
In nutshell, the transition from node $i$ to node $j$ in $\ell$ steps involves two events that the walker moves to block $b$ in $\ell$ steps and lands on a node $j$ selected randomly from block $b$, \ie
\begin{align}
    \label{eq:random_walk_dcSBM}
    \left(\mat{T}_{\text{SBM}} ^{\ell} \right)_{ij} &= \left(\mat{\overline \Lambda} ^{\ell}\right)_{b_i, b_j} \theta^{-}_j,
\end{align}
where $\mat{T}_{\text{SBM}} = (T_{ij} ^{\text{SBM}})$, and $\mat{\overline \Lambda} = (\overline\lambda_{bb'})$ is the normalized propensity matrix, or equivalently the transition matrix of random walks at the block level.
In fact, transition probability from $i$ to $j$ with $\ell=2$ steps is given by
\begin{align}
    \sum_{k=1}^N T^{\text{SBM}} _{ik} T^{\text{SBM}} _{kj} &= \sum_{k=1}^N \left[\overline \lambda_{b_i, b(k)} \lambda_{b(k), b_j} \theta^{-}_k \right] \theta^{-}_j \nonumber \\
                                                           &= \sum_{b=1}^B \left[\overline \lambda_{b_i, b} \lambda_{b, b_j} \sum_{k=1}^N \theta^{-}_{k} \delta(b, b(k))\right] \theta^{-}_j \nonumber \\
                                                           &= \sum_{b=1}^B \left[\overline \lambda_{b_i, b} \lambda_{b, b_j}\right] \theta^{-}_j \nonumber \\
                                                           &= \left(\mat{\overline \Lambda} ^{2}\right)_{b_i, b_j} \theta^{-}_j,
\end{align}
which agrees with Eq.~\eqref{eq:random_walk_dcSBM}.

\section{Embedding of stochastic block model}
\label{sec:community_embedding}

\subsection{Similarity between nodes}

Suppose that a network is generated by SBM, and we embed the network using \textit{node2vec}.
By substituting Eq.~\eqref{eq:random_walk_dcSBM} into Eq.~\eqref{eq:opt_dotsim_graph}, the dot similarity for nodes $i$ and $j$ is given by
\begin{align}
    \label{eq:opt_dotsim_sbm}
    r^{\text{SBM}} _{ij}
    &= \ln \left( \frac{1}{2L}\left(\sum_{\ell=1}^L \mat{T}^\ell _{\text{SBM}}\right)_{ij} + \frac{\pi _j}{2L\pi _i}\left( \sum_{\ell=1}^L \mat{T}^\ell _{\text{SBM}}\right)_{ji} \right) - \ln \pi_j -\ln k \nonumber \\
    & = \ln \left( \frac{1}{2L}\left(\sum_{\ell=1}^L \mat{\overline \Lambda}^\ell \right)_{b_i, b_j}\theta^{-} _j + \frac{\pi _j}{2L\pi _i}\left( \sum_{\ell=1}^L \mat{\overline \Lambda}^\ell \right)_{b_j,b_i} \theta^{-} _i \right) - \ln \pi_j -\ln k.
\end{align}
Now, let us restrict ourselves to undirected networks.
Using Eqs.~\eqref{eq:detailed_balanced_3} and \eqref{eq:pi_graph}, Eq.~\eqref{eq:opt_dotsim_sbm} can be simplified as
\begin{align}
    \label{eq:opt_dotsim_sbm_undirected}
    r^{\text{SBM}} _{ij}
    & = \ln \left(\frac{1}{L}\sum_{\ell=1}^L \mat{\overline \Lambda}^\ell \right)_{b_i, b_j} + \ln \frac{\theta^{-} _j}{s_j} - \ln\frac{k}{2S}.
\end{align}
The key feature of Eq.~\eqref{eq:opt_dotsim_sbm_undirected} is that it makes clear how and to what extent the parameters of SBM are reflected in the final embedding.
More concretely, let us consider degree-\textit{un}corrected SBM.
Using Eq.~\eqref{eq:node_propensity}, dot similarity $r_{ij}^{\text{SBM}}$ for the degree-uncorrected SBM is given by
\begin{align}
    \label{eq:opt_dotsim_ucsbm_undirected}
    r^{\text{ucSBM}} _{ij}
    & = \ln \left(\frac{1}{L}\sum_{\ell=1}^L \mat{\overline \Lambda}^\ell \right)_{b_i, b_j} - \ln n_{b_j}s_j- \ln\frac{k}{2S},
\end{align}
where $n_{b}$ is the number of nodes in block $b$.
Notice that the dot similarity increases as the size $n_{b_j}$ of block and strength of node $j$ decrease, suggesting that nodes in a small block with a small degree tend to have large similarity.

As another example, let us consider the degree-corrected SBM.
Dot similarity $r_{ij}^{\text{SBM}}$ is given by
\begin{align}
    \label{eq:opt_dotsim_dcsbm_undirected}
    r^{\text{dcSBM}} _{ij}
    & = \ln \left[\dfrac{\left(\frac{1}{L}\sum_{\ell=1}^L \mat{\overline \Lambda}^\ell \right)_{b_i, b_j}}{S_{b_j}}\right] - \ln\frac{k}{2S}.
\end{align}
Crucially, $r^{\text{dcSBM}}_{ij}$ does not involve any node-specific features (\eg $s_j$). In other words, the dot similarity $r^{\text{dcSBM}} _{ij}$ is equal within and between the blocks.

\subsection{Markov stability}

Equation~\eqref{eq:opt_dotsim_dcsbm_undirected} provides an interpretation regarding Markov stability~\cite{Delvenne2010}.
Suppose that we run a random walker for an infinite number of steps in a network generated by dcSBM.
Bearing in mind that $\mat{\Lambda}$ is the transition matrix of random walks at block level, the stationary probability $\overline \pi_b$ for block $b$ is given by
\begin{align}
    \label{eq:pi_block}
    \overline \pi_b = (\mat{\overline \Lambda}^\infty )_{b',b}.
\end{align}
Alternatively, $\overline \pi_b$ is given by the sum of $\pi_i$ over all nodes in block $b$, \ie
\begin{align}
    \label{eq:pi_block_2}
    \overline \pi_b = \sum_{i=1}^N \pi_i \delta(b, b_i) = \frac{S_b}{2S}.
\end{align}
Putting Eqs.~\eqref{eq:pi_block} and \eqref{eq:pi_block_2} together, we have
\begin{align}
    \label{eq:pi_block_3}
    S_b = 2S (\mat{\overline \Lambda}^\infty )_{b',b}.
\end{align}
Substituting Eq.~\eqref{eq:pi_block_3} into Eq.~\eqref{eq:opt_dotsim_dcsbm_undirected} yields
\begin{align}
    \label{eq:markov_stability}
    r^{\text{dcSBM}} _{ij}
    & = \ln \dfrac{\left(\frac{1}{L}\sum_{\ell=1}^L \mat{\overline \Lambda}^\ell \right)_{b_i, b_j}}{\left(\mat{\overline \Lambda}^\infty\right)_{b_i, b_j}} - \ln k.
\end{align}
Equation~\eqref{eq:markov_stability} makes clear that
dot similarity $r^{\text{dcSBM}} _{ij}$ indicates the Markov stability of random walks, as measured by the log ratio of the transition probability from block $b_i$ to $b_j$ averaged over $L$ steps against the probability for $\ell=\infty$ steps.

\subsection{Necessary condition for successful clustering}
\label{sec:community_dcsbm}

\textit{node2vec} encodes the community structure modeled by SBM into the final embedding.
Now, let us consider a reverse problem: can we recover communities from the embedding by clustering?

A fundamental assumption of clustering is that clusters are compact and well separated from each other.
From a node-centric viewpoint, a node $i$ is closer to node $j$ in the same cluster than node $j'$ in different clusters.
Therefore, a necessary condition for perfect clustering is
\begin{align}
    \min_{j, b_i = b_j} \vec{u}^\top _i \vec{u} _j > \max_{j', b_i \neq b_j'} \vec{u}^\top _i \vec{u}_{j'},\; \forall 1\leq i \leq N.
\end{align}
For the degree-uncorrected SBM,
\begin{align}
    \label{eq:clustering_cond_ucSBM}
    \min_{j, b_i = b_j} \frac{\left(\frac{1}{L}\sum_{\ell=1}^L \mat{\overline \Lambda}^\ell \right)_{b_i, b_i}}{s_j} > \max_{j', b_i \neq b_{j'}} \frac{\left(\frac{1}{L}\sum_{\ell=1}^L \mat{\overline \Lambda}^\ell \right)_{b_i, b_{j'}}}{s_{j'}},\; \forall 1\leq i \leq N.
\end{align}
Notice that this condition can be easily failed if a node $j$ in the same cluster has a large degree (left-hand side of Eq.~\eqref{eq:clustering_cond_ucSBM}) while node $j'$ in a different cluster has a small degree (right-hand side of Eq.~\eqref{eq:clustering_cond_ucSBM}), which is common in practice.

Now, let us consider the condition for the degree-corrected SBM.
\begin{align}
    \label{eq:clustering_cond_dcSBM}
    \min_{b} \dfrac{\left(\frac{1}{L}\sum_{\ell=1}^L \mat{\overline \Lambda}^\ell \right)_{b, b}}{\left(\mat{\overline \Lambda}^\infty\right)_{b, b}} > \max_{b \neq b'} \dfrac{\left(\frac{1}{L}\sum_{\ell=1}^L \mat{\overline \Lambda}^\ell \right)_{b, b'}}{\left(\mat{\overline \Lambda}^\infty\right)_{b, b'}},\; \forall 1 \leq b \leq B.
\end{align}
Crucially, this condition is irrespective of node-specific features and depends only on random walks at the block level (\ie $\mat{\overline \Lambda}$).
An interesting feature of Eq.~\eqref{eq:clustering_cond_dcSBM} is that it encompasses the criteria for some community detection methods.
For instance, bearing in mind that $\lambda_{b,b'}$ indicates the number of edges from block $b$ to block $b'$, we rewrite $\overline \lambda_{b,b'} = \lambda_{b,b'} / S_b$ (Eq.~\eqref{eq:Tsbm}), where $S_b = S^{-}_b = S^{+}_b$ because the network is undirected.
By setting $\ell=1$ and substituting $\overline \lambda_{b,b'} = \lambda_{b,b'} / S_{b}$ into Eq.~\eqref{eq:clustering_cond_dcSBM}, we have
\begin{align}
    \label{eq:clustering_cond_normalized_cut}
    \min_{b} \dfrac{\lambda _{b, b}}{S_{b}} > \max_{b \neq b'} \dfrac{\lambda _{b, b'}}{S_{b'}},\; \forall 1 \leq b \leq B.
\end{align}
Notice that $\lambda _{b, b}/ S_{b}$ is the criterion of the normalized cut~\cite{Malik2000c}, and the condition (Eq.~\eqref{eq:clustering_cond_normalized_cut}) is always satisfied if the communities correspond to the optimal partitioning for the normalized cut.

\section{Notes}

Thoughts:
\begin{enumerate}
\item While the necessary condition for successful clustering is based on dot similarity, many methods perform clustering based on Euclidean distance. Can we improve clustering performance by using dot similarity instead of Euclidean distance?
\item It seems that the dot similarity does not depend on global statistics of the networks, which implies that embedding is free from resolution limit. If this is true, this feature may warrant the use of graph embedding for clustering? How can we show that embedding is resolution-free?
\item I neglected the stochastic nature of SBM and focus on its expectation of network.
\end{enumerate}

\noindent TODO:
\begin{enumerate}
\item Explore relationship with Modularity, Normalized Cut (any other criteria for community detection that might have a connection?).
\item Argue the resolution limit.
\item Extend the argument to the variants of \textit{node2vec} such as DeepWalk.
\item Numerical simulations for validations.
\end{enumerate}

\printbibliography{}

\end{document} %}}}
